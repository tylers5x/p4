---
title: "Dwellings ML Practice"
format: html
execute:
  echo: true
  warning: false
  message: false
jupyter: python-ds
---

## Setup

```{python}
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

sns.set(style="whitegrid")

url = "https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv"

df = pd.read_csv(url)

df.head()
```

## Task 1 – Living area vs before1980

```{python}
plt.figure()
sns.boxplot(data=df, x="before1980", y="livearea")
plt.xlabel("Built before 1980 (0 = No, 1 = Yes)")
plt.ylabel("livearea (square feet)")
plt.title("Living Area vs. Built Before 1980")
plt.show()
```
## Task 1 – Bathrooms vs before1980

```{python}
plt.figure()
sns.boxplot(data=df, x="before1980", y="numbaths")
plt.xlabel("Built before 1980 (0 = No, 1 = Yes)")
plt.ylabel("numbaths")
plt.title("Number of Bathrooms vs. Built Before 1980")
plt.show()
```
## Task 1 – Bedrooms vs before1980

```{python}
plt.figure()
sns.boxplot(data=df, x="before1980", y="numbdrm")
plt.xlabel("Built before 1980 (0 = No, 1 = Yes)")
plt.ylabel("numbdrm")
plt.title("Number of Bedrooms vs. Built Before 1980")
plt.show()
```

## Task 2 – Classification model for `before1980`
```{python}
# If you ever get "No module named 'sklearn'", install with:
# pip install scikit-learn

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# -----------------------------
# 1. Define features (X) and target (y)
# -----------------------------

# Target: 1 = built before 1980, 0 = 1980 or later
y = df["before1980"]

# Features: drop target, ID column, and yrbuilt (we assume year built is missing)
X = df.drop(
    columns=[
        "before1980",  # target
        "parcel",      # ID, not a real feature
        "yrbuilt"      # would "cheat" because it almost directly reveals the label
    ]
)

# Train / test split (use stratify to keep 0/1 balance)
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

X_train.shape, X_test.shape
```

### Task 2 – Train and evaluate two models

```{python}
# Logistic Regression (linear model, with feature scaling)
log_reg_model = Pipeline([
    ("scaler", StandardScaler()),
    ("model", LogisticRegression(max_iter=1000))
])

# Random Forest (ensemble of decision trees)
rf_model = RandomForestClassifier(
    n_estimators=300,
    max_depth=None,
    random_state=42,
    n_jobs=-1
)

models = {
    "Logistic Regression": log_reg_model,
    "Random Forest": rf_model
}

results = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    results[name] = {"model": model, "accuracy": acc}
    print(f"{name} accuracy: {acc:.3f}")
```

### Task 2 – Choose the final model

```{python}
# Pick the model with the highest test accuracy and keep it for later tasks
best_name = max(results, key=lambda n: results[n]["accuracy"])
final_model = results[best_name]["model"]
best_acc = results[best_name]["accuracy"]
feature_names = X_train.columns.tolist()  # keep names for feature importance in Task 3

print(f"FINAL MODEL: {best_name} (accuracy = {best_acc:.3f})")
```


## Task 3 – Feature importance (justify the model)

```{python}
import numpy as np
from sklearn.inspection import permutation_importance

# Helper: unwrap a Pipeline to get the final estimator if needed
def unwrap_estimator(model):
    if hasattr(model, "named_steps"):  # Pipeline
        return model.named_steps.get("model", model)
    return model

est = unwrap_estimator(final_model)

# 1) Tree model (e.g., RandomForest): use built-in importances
# 2) Linear model (LogisticRegression): use absolute coefficients
# 3) Otherwise: fall back to permutation importance (model-agnostic)
if hasattr(est, "feature_importances_"):
    importances = est.feature_importances_
    importance_method = "Gini importance from RandomForest"
elif hasattr(est, "coef_"):
    coef = est.coef_
    if coef.ndim == 2:  # binary classification -> shape (1, n_features)
        coef = coef[0]
    importances = np.abs(coef)
    importance_method = "Absolute coefficients from Logistic Regression"
else:
    perm = permutation_importance(final_model, X_test, y_test,
                                  n_repeats=10, random_state=42, n_jobs=-1)
    importances = perm.importances_mean
    importance_method = "Permutation importance on the test set"

# Build a DataFrame of importances
fi = pd.DataFrame({"feature": feature_names, "importance": importances})
fi = fi.sort_values("importance", ascending=False).reset_index(drop=True)

# Show top 15 as a bar chart
top_n = 15
fi_top = fi.head(top_n)

plt.figure(figsize=(8, max(4, top_n * 0.35)))
plt.barh(fi_top["feature"][::-1], fi_top["importance"][::-1])
plt.xlabel("Importance")
plt.title(f"Top {top_n} Feature Importances\n({importance_method})")
plt.tight_layout()
plt.show()

# Also print a small table you can reference in text
fi.head(20)
```

## Task 4 – Model evaluation (quality)

```{python}
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, ConfusionMatrixDisplay, roc_curve
)

y_pred = final_model.predict(X_test)

if hasattr(final_model, "predict_proba"):
    y_score = final_model.predict_proba(X_test)[:, 1]
elif hasattr(final_model, "decision_function"):
    y_score = final_model.decision_function(X_test)
else:
    y_score = (y_pred == 1).astype(int)

acc  = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred, zero_division=0)
rec  = recall_score(y_test, y_pred, zero_division=0)
f1w  = f1_score(y_test, y_pred, average="weighted")
auc  = roc_auc_score(y_test, y_score)

pd.DataFrame({
    "metric": ["Accuracy", "Precision", "Recall", "F1 (weighted)", "ROC-AUC"],
    "value":  [acc, prec, rec, f1w, auc]
})
```

```{python}
ConfusionMatrixDisplay.from_predictions(
    y_test, y_pred, normalize="true", cmap="Blues"
)
plt.title("Confusion Matrix (rows = true, cols = predicted)")
plt.show()
```

```{python}
fpr, tpr, _ = roc_curve(y_test, y_score)
plt.figure()
plt.plot(fpr, tpr, label=f"AUC = {auc:.3f}")
plt.plot([0, 1], [0, 1], "--", label="Random guess")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve — Class: Built before 1980 (1)")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```
 
 